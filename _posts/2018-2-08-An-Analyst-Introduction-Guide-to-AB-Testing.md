1) have a key metric for your experiment (click through rate, conversion rate, etc). You can monitor more metrics to make sure you don’t accidentally tank them or something, but you should have one as a goal. (edited)
2) use that to do a power calculation. You input what that metric is now, how many visitors you get, what type of change you’re aiming to get (1% increase? 5%?), % you’re experimenting on (e.g. are you doing 50/50 or 75/25), desired level of power (usually 80%) and you can get how long you need to run this experiment for. If you’re doing a proportion metric, experimentcalculator.com is good for this. (edited)
Two things can pop up 1) it will take a week or two or 2) it will take 10 months. If the latter happens, you may either have to go for a different metric with a higher baseline rate or decide you only care about bigger changes (e.g. “I’m ok that we can’t measure if we increase clicks by 1% because only a 10% or greater increase is meaningful)
Note on 1) revenue is probably the *wrong* metric to pick. It is likely a very skewed distribution which makes stats tests behave poorly
3) run your experiment for the length you’ve planned on. Monitor it in the first few days to make sure nothing exploded. But plan on running it for the length you said. Don’t stop as soon as something is significant or you will get a lot of false positives
4) Confidence interval is more important than the p-value. They have a 1-1 relationship in the sense that if p < .05 the 95% confidence interval does not overlap with 0. But if the CI is wide and very close to zero, you’ve got a lot less evidence than if it’s tiny and far away

Another pitfall - running tons of variants (you will lower your ability to detect a statistical effect and raise the likelihood of a false positive) or trying every possible segment (e.g. did we help US visitors? new visitors? visitors on Saturday?) etc etc again for false positive problem
