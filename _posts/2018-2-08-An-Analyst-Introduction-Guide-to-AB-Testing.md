When I was working at Etsy, I benefited from a very robust A/B testing system. Etsy had been doing A/B testing for more than 6 years (see a great and still relevant 2012 presentation from Dan McKinley, former Principal Engineer, on designing for [continuous experimentation](http://mcfunley.com/design-for-continuous-experimentation). By the time I left, Etsy's in-house experimentation system, called Catapult, had more than 5 data engineers working on it fulltime. Every morning, I was greeted with a homepage that listed all the experiments that Etsy had run in the past four years. When you clicked on one, you got a summary of what the experiment was testing (usually written by the product manager). Numbers for all the key metrics, such as conversion rate and add to cart rate, were already calculated. You could easily add any event that happened on the site and get those rates calculated too. You got to see the % change from the control to treatment with it's accompanying p-value and how many days until we had 80% power to detect a 1% change. We even had beautiful little confidence intervals that changed color based on whether they overlapped with zero! 

And yet sometimes I would spend a majority of my time working on experiments, even though I tried to never be working on more than 3 or 4 at once. How could that take so long when so much was already done for me? The concept of A/B Testing seems pretty simple. A classic example is you change the color of a button and measuring if the click-rate changes. Assuming your assignment of visitors and data collection is working, all you need to do is run a proportion test, right? And if you already have the proportion test calculated, why is a data scientist even needed? Maybe you need one if you want to do some fancy techniques like multi-armed bandits, but how can classic, frequentist A/B Testing be a challenge? 

### Basic A/B Testing Rules

1) **Have one key metric for your experiment**. You can (and should!) monitor more metrics to make sure you don’t accidentally tank them, but you should have one as a goal. Revenue is probably the **wrong** metric to pick. It is likely a very skewed distribution which makes traditional statistics tests behave poorly. See my discussion in [my A/B testing talk](https://www.youtube.com/watch?v=SF-ryGgLOgQ) (around the 23 minute mark). I generally recommend proportion metrics, as 1) many times you care more about the number of people doing something then how much they do it and 2) you don't have to deal with messy things like outliers and standard deviations changing along with means. 

    Why only one metric? Once you start testing many metrics, you run into an increased false positive rate from multiple testing. You can use [correction methods](https://en.wikipedia.org/wiki/Bonferroni_correction) that will counteract this problem, but they make each test more conservative, so you become less likely to detect a difference in any given test. One metric also makes decision-making clearer - what would happen if you have three "equally important" metrics, and two go up while the other two go down?  
  
2) **Use that key metric do a power calculation**. You'll need the current rate (if a proportion metric) or mean and standard deviation of your key metric, how many visitors you get daily, what type of change you’re aiming to get (1% increase? 5%?), the percentage of people you’ll be allocating to your new version (e.g. are you doing 50/50 or 75/25 split), desired level of power (usually 80%) and the significance threshold (usually 95% or 90%), and you can get how long you need to run this experiment for. If you’re doing a proportion metric, [experimentcalculator.com](experimentcalculator.com) is good for this. 
  
   Two things will generally happen: 1) you'll get that it will take a few days or weeks or 2) you'll get that it will take 3  years, 5 months, and 23 days. If the latter happens, you may either have to go for a different metric with a higher baseline  rate or decide you only care about bigger changes (e.g. “I’m ok that we can’t measure if we increase clicks by 1% because  only a 10% or greater increase is meaningful). If you want to learn more about power, check out Julia Silge's [excellent post](https://juliasilge.com/blog/ab-testing/), which includes a [shiny app](https://juliasilge.shinyapps.io/power-app/) for seeing how your power level changes with your effect size and population. Warning: do not do a post-hoc power analysis http://www.vims.edu/people/hoenig_jm/pubs/hoenig2.pdf.

3) **Run your experiment for the length you’ve planned on**. Monitor it in the first few days to make sure nothing exploded. But plan on running it for the length you said. Don’t stop as soon as something is significant or you will get a lot of false positives. See the review section in Dave Robinson's Bayesian A/B Testing [blog post](http://varianceexplained.org/r/bayesian-ab-testing/). Don't be another p-hacking statistic:

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">When p-hacking is taken into account, 73% of A/B tests on Optimizely have no effect <a href="https://t.co/YNsx6IGBfk">https://t.co/YNsx6IGBfk</a> <a href="https://t.co/xeLnOal3ba">pic.twitter.com/xeLnOal3ba</a></p>&mdash; Josh Kalla (@j_kalla) <a href="https://twitter.com/j_kalla/status/1019629420158447616?ref_src=twsrc%5Etfw">July 18, 2018</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

4) **Pay more attention to confidence intervals than p-values**. They have a 1-1 relationship such that if the p-value is less than .05, the 95% confidence interval does not overlap with 0. But if the confidence interval is wide and very close to zero, you’ve got a lot less evidence of a change than if it’s tiny and far away. 

5) **Don't run tons of variants**. Say you want . How do you pick one? Well, that's what A/B Testing is for, right? Wrong. You will lower your ability to detect a statistical effect, as each group will have fewer people in it, and raise the likelihood of a false positive if you simply test the control against each treatment group. 

6) **Don't try to look for differences for every possible segment.** If your test doesn't work overall, it can be tempting to hold out hope that it actually did, just not for everyone. Or even if your A/B tests did succeed, you may want to know if it was driven by a big change in one segment. Did we help US visitors? New visitors? Visitors on Saturday? Down that road lies the madness of false positives from multiple testing, also known as detecting differences in health [based on astrological signs](https://pdfs.semanticscholar.org/d00a/678b0d09eaef4902c778821c52dc5ac53e58.pdf). 

7) **Check that there's not bucketing skew** (also known as sample ratio mismatch). This is where the split of people between your variants does not match what you planned. For example, maybe you wanted to split people between the control and treatment 50/50 but after a few days, you find 40% are in the treatment and 60% in the control. That's a problem! If you have lots of users, even a difference of a couple thousand can indicate a problem with your set-up. To check if you have an issue, run a [proportion test](https://www.medcalc.org/calc/test_one_proportion.php) and check your p-value. 

8) **Don't overcomplicate your methods**. Maybe you have engineers who've read about [multi-armed bandit testing](http://stevehanov.ca/blog/index.php?id=132), stats nerds who want to use Bayesian methods, or product managers who want the key metric to be a complicated sequence of behaviors. If you're just starting out A/B testing methods, focus on getting the basic, frequentist methods right. Even after a few years, it usually is better to invest in experiment design and education rather fancy statistical methods. 

9) **Be careful of launching things just because they "don't hurt"**. There may actually be a negative change that's too small to detect.  

10) **Have a data scientist/analyst involed in the whole process.** As Sir R. A. Fisher once said “To consult the statistician after an experiment is finished is often merely to ask him to conduct a post mortem examination. [They] can perhaps say what the experiment died of.” If a team tries to bring you in a data scientist launched an experiment, they may find the data doesn't exist to measure their key metric, the test is severely underpowered, or there's a design flaw that means you can't draw any conclusions. 

11) **Only include people in your analysis who could have been affected by the change**. https://onedrive.live.com/view.aspx?resid=8612090E610871E4!287400&ithint=file%2cdocx&app=Word&authkey=!AOW7nw7IZ4STtgk. 

12) **Change only one or a few things per test**. 
